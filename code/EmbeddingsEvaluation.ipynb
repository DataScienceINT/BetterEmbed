{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5389dd5-7abb-4727-8afe-666ffd068a1f",
   "metadata": {},
   "source": [
    "# Define evaluator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d5a451c-986c-4a5c-830e-d5a0a3010fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer  \n",
    "import numpy as np \n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "import json\n",
    "\n",
    "class EmbeddingEvaluator:\n",
    "    def __init__(self, embedding_model_name, k=5):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with the specified embedding model and k value for top-k retrieval.\n",
    "        :param embedding_model_name: Name of the pre-trained embedding model.\n",
    "        :param k: Number of top passages to retrieve.\n",
    "        \"\"\"\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "        self.k = k\n",
    "        self.metrics_store = []\n",
    "\n",
    "    def load_dataset(self, file_path):\n",
    "        \"\"\"\n",
    "        Load dataset from JSON file and return questions and answers.\n",
    "        :param file_path: Path to the JSON dataset file.\n",
    "        :return: Tuple of (questions, answers)\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        questions = []\n",
    "        answers = []\n",
    "\n",
    "        for entry in data.values():\n",
    "            questions.append(entry[\"question\"])\n",
    "            answers.extend(entry[\"answers\"])\n",
    "\n",
    "        return questions, answers\n",
    "\n",
    "    def generate_embeddings(self, texts):\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts using the pre-trained embedding model.\n",
    "        :param texts: List of text strings.\n",
    "        :return: Embeddings as tensors.\n",
    "        \"\"\"\n",
    "        return self.embedding_model.encode(texts, convert_to_tensor=True)\n",
    "\n",
    "    def retrieve_top_k(self, question_embeddings, answer_embeddings):\n",
    "        \"\"\"\n",
    "        Retrieve the top-k most relevant answers based on cosine similarity.\n",
    "        :param question_embeddings: List of question embeddings.\n",
    "        :param answer_embeddings: List of answer embeddings.\n",
    "        :return: Top-k indices for each question.\n",
    "        \"\"\"\n",
    "        cosine_sim = cosine_similarity(question_embeddings, answer_embeddings)\n",
    "        top_k_indices = np.argsort(cosine_sim, axis=1)[:, ::-1][:, :self.k]\n",
    "        return top_k_indices\n",
    "\n",
    "    def compute_metrics(self, dataset_name, relevant_indices, top_k_indices):\n",
    "        \"\"\"\n",
    "        Compute MRR, Precision@k, and Recall@k for the current dataset.\n",
    "        :param dataset_name: Name or path of the dataset being evaluated.\n",
    "        :param relevant_indices: List of relevant indices (correct answers) for each question.\n",
    "        :param top_k_indices: List of top-k retrieved indices for each question.\n",
    "        :return: Tuple of MRR, Precision@k, and Recall@k.\n",
    "        \"\"\"\n",
    "        mrr = self.mean_reciprocal_rank(relevant_indices, top_k_indices)\n",
    "        precision = self.precision_at_k(relevant_indices, top_k_indices)\n",
    "        recall = self.recall_at_k(relevant_indices, top_k_indices)\n",
    "\n",
    "        # Store the metrics for this dataset\n",
    "        self.metrics_store.append({\n",
    "            \"Model\": self.embedding_model_name,\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"MRR\": mrr,\n",
    "            \"Precision@k\": precision,\n",
    "            \"Recall@k\": recall\n",
    "        })\n",
    "\n",
    "        return mrr, precision, recall\n",
    "\n",
    "    def mean_reciprocal_rank(self, relevant_indices, top_k_indices):\n",
    "        \"\"\"\n",
    "        Average of the reciprocal ranks of the first correct answer.\n",
    "        \"\"\"\n",
    "        mrr_total = 0.0\n",
    "        for i, relevant in enumerate(relevant_indices):\n",
    "            for rank, retrieved in enumerate(top_k_indices[i]):\n",
    "                if retrieved in relevant:\n",
    "                    mrr_total += 1.0 / (rank + 1)\n",
    "                    break\n",
    "        return mrr_total / len(relevant_indices)\n",
    "\n",
    "    def precision_at_k(self, relevant_indices, top_k_indices):\n",
    "        \"\"\"\n",
    "        Proportion of retrieved answers in the top k results that are actually correct\n",
    "        \"\"\"\n",
    "        precision_total = 0.0\n",
    "        for i, relevant in enumerate(relevant_indices):\n",
    "            retrieved_set = set(top_k_indices[i][:self.k])\n",
    "            relevant_set = set(relevant)\n",
    "            precision_total += len(retrieved_set.intersection(relevant_set)) / self.k\n",
    "        return precision_total / len(relevant_indices)\n",
    "\n",
    "    def recall_at_k(self, relevant_indices, top_k_indices):\n",
    "        \"\"\"\n",
    "        Proportion of correct answers retrieved in the top k results\n",
    "        \"\"\"\n",
    "        recall_total = 0.0\n",
    "        for i, relevant in enumerate(relevant_indices):\n",
    "            retrieved_set = set(top_k_indices[i][:self.k])\n",
    "            relevant_set = set(relevant)\n",
    "            recall_total += len(retrieved_set.intersection(relevant_set)) / len(relevant_set)\n",
    "        return recall_total / len(relevant_indices)\n",
    "\n",
    "    def evaluate(self, dataset_file_paths):\n",
    "        \"\"\"\n",
    "        Evaluate the embedding model on multiple datasets and compute metrics for each.\n",
    "        :param dataset_file_paths: List of dataset file paths.\n",
    "        \"\"\"\n",
    "        for dataset_file_path in dataset_file_paths:\n",
    "            questions, answers = self.load_dataset(dataset_file_path)\n",
    "\n",
    "            # Generate embeddings for questions and answers\n",
    "            question_embeddings = self.generate_embeddings(questions)\n",
    "            answer_embeddings = self.generate_embeddings(answers)\n",
    "\n",
    "            # Retrieve the top-k results for each question\n",
    "            top_k_indices = self.retrieve_top_k(question_embeddings, answer_embeddings)\n",
    "\n",
    "            # Generate relevant indices (assuming you have the correct answer indices)\n",
    "            relevant_indices = self.get_relevant_indices(dataset_file_path, answers)\n",
    "\n",
    "            # Compute and return the evaluation metrics for the current dataset\n",
    "            mrr, precision, recall = self.compute_metrics(dataset_file_path, relevant_indices, top_k_indices)\n",
    "            print(f\"Dataset: {dataset_file_path}, MRR: {mrr:.3f}, Precision@{self.k}: {precision:.3f}, Recall@{self.k}: {recall:.3f}\")\n",
    "\n",
    "    def get_relevant_indices(self, dataset_file_path, answers):\n",
    "        \"\"\"\n",
    "        Get the relevant indices (correct answers) for each question.\n",
    "        :param dataset_file_path: Path to the dataset file.\n",
    "        :param answers: List of all possible answers.\n",
    "        :return: List of relevant indices for each question.\n",
    "        \"\"\"\n",
    "        with open(dataset_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        relevant_indices = []\n",
    "        for entry in data.values():\n",
    "            relevant = [answers.index(ans) for ans in entry[\"answers\"]]\n",
    "            relevant_indices.append(relevant)\n",
    "\n",
    "        return relevant_indices\n",
    "\n",
    "    def get_metrics_summary(self):\n",
    "        \"\"\"\n",
    "        Retrieve the stored metrics across all evaluated datasets.\n",
    "        :return: List of metric dictionaries for each dataset.\n",
    "        \"\"\"\n",
    "        return self.metrics_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df3c6297-6acf-469a-b845-0332ae3885fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator usage function\n",
    "def evaluate_model(model_name, datasets):\n",
    "    # Initialize the evaluator with a pre-trained Sentence-BERT model\n",
    "    evaluator = EmbeddingEvaluator(model_name, k=5)\n",
    "\n",
    "    # Evaluate the model on all datasets\n",
    "    evaluator.evaluate(datasets)\n",
    "\n",
    "    # Retrieve stored metrics\n",
    "    return(evaluator.get_metrics_summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a60cc5-176a-4bea-b36a-01c126dd1ab1",
   "metadata": {},
   "source": [
    "# Evaluate embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "780b8d4a-0d9d-451e-ba46-f743ecc330e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['bioasq_test_set.json', 'pubmedqa_test_set.json']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c09f01f-762a-43ba-8196-9516e09d2867",
   "metadata": {},
   "source": [
    "The BioASQ test set dataset is a random 20% extract for the BioQSA dataset 12b, it contained about a 1000 questions and their corresponding answers.\n",
    "The PubMedQA test set dataset is the entire PubMed QA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a5f673-1d9d-4943-b858-0d7e9c74978d",
   "metadata": {},
   "source": [
    "## General model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d885105a-e3a6-42e8-b44d-351027101b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: bioasq_test_set.json, MRR: 0.928, Precision@5: 0.722, Recall@5: 0.549\n",
      "Dataset: pubmedqa_test_set.json, MRR: 0.925, Precision@5: 0.192, Recall@5: 0.958\n"
     ]
    }
   ],
   "source": [
    "metrics_general = evaluate_model ('BAAI/bge-base-en-v1.5', datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5e3cab-2ae6-402a-9119-a4247931ec1f",
   "metadata": {},
   "source": [
    "## PubMed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "090b8dee-eef6-486a-9910-3877cd8cb953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: bioasq_test_set.json, MRR: 0.809, Precision@5: 0.590, Recall@5: 0.420\n",
      "Dataset: pubmedqa_test_set.json, MRR: 0.936, Precision@5: 0.195, Recall@5: 0.973\n"
     ]
    }
   ],
   "source": [
    "metrics_pubmed = evaluate_model ('neuml/pubmedbert-base-embeddings', datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7a151a-a06b-45b5-a75e-90a2567220bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7f9828e-67a1-406c-baab-2d48e558b09f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6acf4b6-3ee5-44af-b5fb-077a5e4a11be",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embeddings",
   "language": "python",
   "name": "embeddings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
