{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "536a9771-5908-448c-95ce-42763a8bdb35",
   "metadata": {},
   "source": [
    "# Evaluation of different embedding models on biomedical QA\n",
    "Author: Marie Corradi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5389dd5-7abb-4727-8afe-666ffd068a1f",
   "metadata": {},
   "source": [
    "## Define evaluator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d5a451c-986c-4a5c-830e-d5a0a3010fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcorradi/.conda/envs/embeddings/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from embeddings_training import ContrastiveAutoencoder\n",
    "import numpy as np \n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "import json\n",
    "import torch\n",
    "\n",
    "class EmbeddingEvaluator:\n",
    "    def __init__(self, model_type, model_path=None, embedding_model_name=None, k=5):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with the specified model type.\n",
    "        :param model_type: Either 'custom' for a trained model or 'pretrained' for a SentenceTransformer model.\n",
    "        :param model_path: Path to the trained model (for custom model).\n",
    "        :param embedding_model_name: Name of the pre-trained SentenceTransformer model (for pretrained model).\n",
    "        :param k: Number of top passages to retrieve.\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        self.k = k\n",
    "        self.metrics_store = []\n",
    "        \n",
    "        # Load the appropriate model based on model type\n",
    "        if model_type == \"pretrained\":\n",
    "            self.embedding_model_name = embedding_model_name\n",
    "            self.model = SentenceTransformer(embedding_model_name)  # Load SentenceTransformer model\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "\n",
    "    def load_dataset(self, file_path):\n",
    "        \"\"\"\n",
    "        Load dataset from JSON file and return questions and answers.\n",
    "        :param file_path: Path to the JSON dataset file.\n",
    "        :return: Tuple of (questions, answers)\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        questions = []\n",
    "        answers = []\n",
    "\n",
    "        for entry in data.values():\n",
    "            questions.append(entry[\"question\"])\n",
    "            answers.extend(entry[\"answers\"])\n",
    "\n",
    "        return questions, answers\n",
    "\n",
    "    def generate_embeddings(self, texts):\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts using the pre-trained embedding model or custom model.\n",
    "        :param texts: List of text strings.\n",
    "        :return: Embeddings as tensors.\n",
    "        \"\"\"\n",
    "        if self.model_type == \"pretrained\":\n",
    "            return self.model.encode(texts, convert_to_tensor=True)  # SentenceTransformer's encode method\n",
    "\n",
    "    def retrieve_top_k(self, question_embeddings, answer_embeddings):\n",
    "        \"\"\"\n",
    "        Retrieve the top-k most relevant answers based on cosine similarity.\n",
    "        :param question_embeddings: List of question embeddings.\n",
    "        :param answer_embeddings: List of answer embeddings.\n",
    "        :return: Top-k indices for each question.\n",
    "        \"\"\"\n",
    "        # Check if the embeddings are on GPU and move them to CPU if necessary\n",
    "        if question_embeddings.device != torch.device('cpu'):\n",
    "            question_embeddings = question_embeddings.cpu().detach().numpy()\n",
    "        if answer_embeddings.device != torch.device('cpu'):\n",
    "            answer_embeddings = answer_embeddings.cpu().detach().numpy()\n",
    "    \n",
    "        # Convert to NumPy arrays for cosine similarity calculation\n",
    "        #question_embeddings = question_embeddings.numpy()\n",
    "        #answer_embeddings = answer_embeddings.numpy()\n",
    "\n",
    "        cosine_sim = cosine_similarity(question_embeddings, answer_embeddings)\n",
    "        top_k_indices = np.argsort(cosine_sim, axis=1)[:, ::-1][:, :self.k]\n",
    "        return top_k_indices\n",
    "\n",
    "    def compute_metrics(self, dataset_name, relevant_indices, top_k_indices):\n",
    "        \"\"\"\n",
    "        Compute MRR, Precision@k, and Recall@k for the current dataset.\n",
    "        :param dataset_name: Name or path of the dataset being evaluated.\n",
    "        :param relevant_indices: List of relevant indices (correct answers) for each question.\n",
    "        :param top_k_indices: List of top-k retrieved indices for each question.\n",
    "        :return: Tuple of MRR, Precision@k, and Recall@k.\n",
    "        \"\"\"\n",
    "        mrr = self.mean_reciprocal_rank(relevant_indices, top_k_indices)\n",
    "        precision = self.precision_at_k(relevant_indices, top_k_indices)\n",
    "        recall = self.recall_at_k(relevant_indices, top_k_indices)\n",
    "\n",
    "        # Store the metrics for this dataset\n",
    "        self.metrics_store.append({\n",
    "            \"Model\": self.model_type if self.model_type == \"custom\" else self.embedding_model_name,\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"MRR\": mrr,\n",
    "            \"Precision@k\": precision,\n",
    "            \"Recall@k\": recall\n",
    "        })\n",
    "\n",
    "        return mrr, precision, recall\n",
    "\n",
    "    def mean_reciprocal_rank(self, relevant_indices, top_k_indices):\n",
    "        # Rank of the first relevant result returned\n",
    "        mrr_total = 0.0\n",
    "        for i, relevant in enumerate(relevant_indices):\n",
    "            for rank, retrieved in enumerate(top_k_indices[i]):\n",
    "                if retrieved in relevant:\n",
    "                    mrr_total += 1.0 / (rank + 1)\n",
    "                    break\n",
    "        return mrr_total / len(relevant_indices)\n",
    "\n",
    "    def precision_at_k(self, relevant_indices, top_k_indices):\n",
    "        # How many of the top k retrieved results are relevant\n",
    "        precision_total = 0.0\n",
    "        for i, relevant in enumerate(relevant_indices):\n",
    "            retrieved_set = set(top_k_indices[i][:self.k])\n",
    "            relevant_set = set(relevant)\n",
    "            precision_total += len(retrieved_set.intersection(relevant_set)) / self.k\n",
    "        return precision_total / len(relevant_indices)\n",
    "\n",
    "    def recall_at_k(self, relevant_indices, top_k_indices):\n",
    "        # How many of the relevant items are retrieved within the top k results\n",
    "        recall_total = 0.0\n",
    "        for i, relevant in enumerate(relevant_indices):\n",
    "            retrieved_set = set(top_k_indices[i][:self.k])\n",
    "            relevant_set = set(relevant)\n",
    "            recall_total += len(retrieved_set.intersection(relevant_set)) / len(relevant_set)\n",
    "        return recall_total / len(relevant_indices)\n",
    "\n",
    "    def evaluate(self, dataset_file_paths):\n",
    "        \"\"\"\n",
    "        Evaluate the embedding model on multiple datasets and compute metrics for each.\n",
    "        :param dataset_file_paths: List of dataset file paths.\n",
    "        \"\"\"\n",
    "        for dataset_file_path in dataset_file_paths:\n",
    "            questions, answers = self.load_dataset(dataset_file_path)\n",
    "\n",
    "            # Generate embeddings for questions and answers\n",
    "            question_embeddings = self.generate_embeddings(questions)\n",
    "            answer_embeddings = self.generate_embeddings(answers)\n",
    "\n",
    "            # Retrieve the top-k results for each question\n",
    "            top_k_indices = self.retrieve_top_k(question_embeddings, answer_embeddings)\n",
    "\n",
    "            # Generate relevant indices \n",
    "            relevant_indices = self.get_relevant_indices(dataset_file_path, answers)\n",
    "\n",
    "            # Compute and return the evaluation metrics for the current dataset\n",
    "            mrr, precision, recall = self.compute_metrics(dataset_file_path, relevant_indices, top_k_indices)\n",
    "            print(f\"Dataset: {dataset_file_path}, MRR: {mrr:.3f}, Precision@{self.k}: {precision:.3f}, Recall@{self.k}: {recall:.3f}\")\n",
    "\n",
    "    def get_relevant_indices(self, dataset_file_path, answers):\n",
    "        \"\"\"\n",
    "        Get the relevant indices (correct answers) for each question.\n",
    "        :param dataset_file_path: Path to the dataset file.\n",
    "        :param answers: List of all possible answers.\n",
    "        :return: List of relevant indices for each question.\n",
    "        \"\"\"\n",
    "        with open(dataset_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        relevant_indices = []\n",
    "        for entry in data.values():\n",
    "            relevant = [answers.index(ans) for ans in entry[\"answers\"]]\n",
    "            relevant_indices.append(relevant)\n",
    "\n",
    "        return relevant_indices\n",
    "\n",
    "    def get_metrics_summary(self):\n",
    "        \"\"\"\n",
    "        Retrieve the stored metrics across all evaluated datasets.\n",
    "        :return: List of metric dictionaries for each dataset.\n",
    "        \"\"\"\n",
    "        return self.metrics_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df3c6297-6acf-469a-b845-0332ae3885fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator usage function\n",
    "def evaluate_model(datasets,model_type, model_path=None, embedding_model_name=None, k=5):\n",
    "    # Initialize the evaluator with a pre-trained Sentence-BERT model\n",
    "    evaluator = EmbeddingEvaluator(model_type, model_path=model_path, embedding_model_name=embedding_model_name, k=k)\n",
    "\n",
    "    # Evaluate the model on all datasets\n",
    "    evaluator.evaluate(datasets)\n",
    "\n",
    "    # Retrieve stored metrics\n",
    "    return(evaluator.get_metrics_summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a60cc5-176a-4bea-b36a-01c126dd1ab1",
   "metadata": {},
   "source": [
    "## Evaluate embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "780b8d4a-0d9d-451e-ba46-f743ecc330e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['/home/mcorradi/researchdrive/BetterEmbedData/bioasq_test_set.json', '/home/mcorradi/researchdrive/BetterEmbedData/pubmedqa_test_set.json']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c09f01f-762a-43ba-8196-9516e09d2867",
   "metadata": {},
   "source": [
    "The BioASQ test set dataset is a random 20% extract for the BioQSA dataset 12b, it contained about a 1000 questions and their corresponding answers.\n",
    "The PubMedQA test set dataset is the entire PubMed QA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a5f673-1d9d-4943-b858-0d7e9c74978d",
   "metadata": {},
   "source": [
    "### General model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d885105a-e3a6-42e8-b44d-351027101b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: /home/mcorradi/researchdrive/BetterEmbedData/bioasq_test_set.json, MRR: 0.932, Precision@5: 0.720, Recall@5: 0.549\n",
      "Dataset: /home/mcorradi/researchdrive/BetterEmbedData/pubmedqa_test_set.json, MRR: 0.925, Precision@5: 0.192, Recall@5: 0.958\n"
     ]
    }
   ],
   "source": [
    "metrics_general = evaluate_model(datasets,model_type='pretrained',embedding_model_name = 'BAAI/bge-base-en-v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d92e2190-c8f2-49eb-a471-3d7f993ae5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: /home/mcorradi/researchdrive/BetterEmbedData/bioasq_test_set.json, MRR: 0.895, Precision@1: 0.895, Recall@1: 0.199\n",
      "Dataset: /home/mcorradi/researchdrive/BetterEmbedData/pubmedqa_test_set.json, MRR: 0.901, Precision@1: 0.901, Recall@1: 0.901\n"
     ]
    }
   ],
   "source": [
    "metrics_general_1 = evaluate_model(datasets,model_type='pretrained',embedding_model_name = 'BAAI/bge-base-en-v1.5', k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5470a3cb-5d21-4424-8d58-52efca5b52ca",
   "metadata": {},
   "source": [
    "### Nomic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c090e-a741-4457-892f-43e8327561dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_nomic = evaluate_model(datasets,model_type='pretrained',embedding_model_name = 'BAAI/bge-base-en-v1.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5e3cab-2ae6-402a-9119-a4247931ec1f",
   "metadata": {},
   "source": [
    "### PubMed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "090b8dee-eef6-486a-9910-3877cd8cb953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: /home/mcorradi/researchdrive/BetterEmbedData/bioasq_test_set.json, MRR: 0.809, Precision@5: 0.590, Recall@5: 0.421\n",
      "Dataset: /home/mcorradi/researchdrive/BetterEmbedData/pubmedqa_test_set.json, MRR: 0.936, Precision@5: 0.195, Recall@5: 0.973\n"
     ]
    }
   ],
   "source": [
    "metrics_pubmed = evaluate_model(datasets,model_type='pretrained',embedding_model_name = 'neuml/pubmedbert-base-embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "443aed1d-2b19-4387-8dfb-49cf735abc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: /home/mcorradi/researchdrive/BetterEmbedData/bioasq_test_set.json, MRR: 0.766, Precision@1: 0.766, Recall@1: 0.157\n",
      "Dataset: /home/mcorradi/researchdrive/BetterEmbedData/pubmedqa_test_set.json, MRR: 0.910, Precision@1: 0.910, Recall@1: 0.910\n"
     ]
    }
   ],
   "source": [
    "metrics_pubmed_1 = evaluate_model(datasets,model_type='pretrained',embedding_model_name = 'neuml/pubmedbert-base-embeddings', k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef863582-a203-4129-915c-c04f3748e096",
   "metadata": {},
   "source": [
    "### MedEmbed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeb9ffc9-5f09-41a4-829c-688ff4e83b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 3.2.0, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: /home/mcorradi/researchdrive/BetterEmbedData/bioasq_test_set.json, MRR: 0.928, Precision@5: 0.720, Recall@5: 0.546\n",
      "Dataset: /home/mcorradi/researchdrive/BetterEmbedData/pubmedqa_test_set.json, MRR: 0.955, Precision@5: 0.195, Recall@5: 0.976\n"
     ]
    }
   ],
   "source": [
    "metrics_med = evaluate_model(datasets,model_type='pretrained',embedding_model_name = 'abhinand/MedEmbed-base-v0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3eca6c1a-1994-43d8-968c-d990e7c34ed3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 3.2.0, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: /home/mcorradi/researchdrive/BetterEmbedData/bioasq_test_set.json, MRR: 0.891, Precision@1: 0.891, Recall@1: 0.198\n",
      "Dataset: /home/mcorradi/researchdrive/BetterEmbedData/pubmedqa_test_set.json, MRR: 0.939, Precision@1: 0.939, Recall@1: 0.939\n"
     ]
    }
   ],
   "source": [
    "metrics_med_1 = evaluate_model(datasets,model_type='pretrained',embedding_model_name = 'abhinand/MedEmbed-base-v0.1', k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80b7d871-1a18-4a17-a788-496917b64978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcorradi/.conda/envs/embeddings/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from embeddings_training import ContrastiveAutoencoder\n",
    "from colbert import Indexer, Searcher\n",
    "from colbert.infra import Run, ColBERTConfig\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "\n",
    "class EmbeddingEvaluator:\n",
    "    def __init__(self, model_type, model_path=None, embedding_model_name=None, k=5, index_name=\"default\"):\n",
    "        self.model_type = model_type\n",
    "        self.k = k\n",
    "        self.index_name = index_name\n",
    "        self.metrics_store = []\n",
    "\n",
    "        # Initialize models based on model type\n",
    "        if model_type == \"custom\":\n",
    "            if model_path is None:\n",
    "                raise ValueError(\"model_path must be provided for a custom model.\")\n",
    "            self.model = ContrastiveAutoencoder.from_pretrained(model_path)\n",
    "        elif model_type == \"pretrained\":\n",
    "            if embedding_model_name is None:\n",
    "                raise ValueError(\"embedding_model_name must be provided for a pre-trained model.\")\n",
    "            self.model = SentenceTransformer(embedding_model_name)\n",
    "        elif model_type == \"colbert\":\n",
    "            if embedding_model_name is None:\n",
    "                raise ValueError(\"embedding_model_name must be provided for a ColBERT model.\")\n",
    "            self.config = ColBERTConfig(doc_maxlen=512, nbits=2)\n",
    "            self.indexer = Indexer(checkpoint=embedding_model_name, config=self.config)\n",
    "            self.searcher = None  # Initialize later after indexing answers\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "        \n",
    "        if model_type in [\"custom\", \"pretrained\"]:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            self.model.to(device)\n",
    "\n",
    "    def load_dataset(self, file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        questions, answers = [], []\n",
    "        for entry in data.values():\n",
    "            questions.append(entry[\"question\"])\n",
    "            answers.extend(entry[\"answers\"])\n",
    "        return questions, answers\n",
    "\n",
    "    def index_answers(self, answers):\n",
    "        \"\"\"Indexes the answers once for efficient retrieval in ColBERT.\"\"\"\n",
    "        if self.model_type == \"colbert\":\n",
    "            self.indexer.index(name=self.index_name, collection=answers, overwrite=\"force_silent_overwrite\")\n",
    "            self.searcher = Searcher(index=self.index_name, config=self.config)\n",
    "\n",
    "    def generate_question_embeddings(self, questions):\n",
    "        \"\"\"Generates embeddings for questions based on model type.\"\"\"\n",
    "        if self.model_type in [\"custom\", \"pretrained\"]:\n",
    "            return self.model.encode(questions, convert_to_tensor=True)\n",
    "        elif self.model_type == \"colbert\":\n",
    "            # For ColBERT, retrieve based on query without generating embeddings directly\n",
    "            return questions\n",
    "\n",
    "    def retrieve_top_k(self, questions, answer_embeddings=None):\n",
    "        \"\"\"Retrieve the top-k most relevant answers for each question.\"\"\"\n",
    "        if self.model_type == \"colbert\":\n",
    "            top_k_docs = [self.searcher.search(query, k=self.k) for query in questions]\n",
    "            return [[doc[0] for doc in docs] for docs in top_k_docs]\n",
    "        else:\n",
    "            # For custom and pretrained models, use cosine similarity\n",
    "            if questions.device != torch.device('cpu'):\n",
    "                questions = questions.cpu().detach().numpy()\n",
    "            if answer_embeddings.device != torch.device('cpu'):\n",
    "                answer_embeddings = answer_embeddings.cpu().detach().numpy()\n",
    "            cosine_sim = cosine_similarity(questions, answer_embeddings)\n",
    "            return np.argsort(cosine_sim, axis=1)[:, ::-1][:, :self.k]\n",
    "\n",
    "    def compute_metrics(self, dataset_name, relevant_indices, top_k_indices):\n",
    "        mrr = self.mean_reciprocal_rank(relevant_indices, top_k_indices)\n",
    "        precision = self.precision_at_k(relevant_indices, top_k_indices)\n",
    "        recall = self.recall_at_k(relevant_indices, top_k_indices)\n",
    "        self.metrics_store.append({\n",
    "            \"Model\": self.model_type if self.model_type == \"custom\" else self.embedding_model_name,\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"MRR\": mrr,\n",
    "            \"Precision@k\": precision,\n",
    "            \"Recall@k\": recall\n",
    "        })\n",
    "        return mrr, precision, recall\n",
    "\n",
    "    def evaluate(self, dataset_file_paths):\n",
    "        \"\"\"Evaluate the embedding model on multiple datasets.\"\"\"\n",
    "        for dataset_file_path in dataset_file_paths:\n",
    "            questions, answers = self.load_dataset(dataset_file_path)\n",
    "\n",
    "            # ColBERT: Index answers once and reuse\n",
    "            if self.model_type == \"colbert\":\n",
    "                self.index_answers(answers)\n",
    "\n",
    "            # Generate question embeddings or use questions as is for ColBERT\n",
    "            question_embeddings = self.generate_question_embeddings(questions)\n",
    "            answer_embeddings = self.generate_embeddings(answers) if self.model_type != \"colbert\" else answers\n",
    "\n",
    "            # Retrieve the top-k results for each question\n",
    "            top_k_indices = self.retrieve_top_k(question_embeddings, answer_embeddings)\n",
    "\n",
    "            # Generate relevant indices\n",
    "            relevant_indices = self.get_relevant_indices(dataset_file_path, answers)\n",
    "\n",
    "            # Compute and display metrics\n",
    "            mrr, precision, recall = self.compute_metrics(dataset_file_path, relevant_indices, top_k_indices)\n",
    "            print(f\"Dataset: {dataset_file_path}, MRR: {mrr:.3f}, Precision@{self.k}: {precision:.3f}, Recall@{self.k}: {recall:.3f}\")\n",
    "\n",
    "    def get_relevant_indices(self, dataset_file_path, answers):\n",
    "        with open(dataset_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        relevant_indices = []\n",
    "        for entry in data.values():\n",
    "            relevant = [answers.index(ans) for ans in entry[\"answers\"]]\n",
    "            relevant_indices.append(relevant)\n",
    "        return relevant_indices\n",
    "\n",
    "    def get_metrics_summary(self):\n",
    "        return self.metrics_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71f6d97a-ff3a-4939-b988-7c7625a0d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator usage function\n",
    "def evaluate_model(datasets,model_type, model_path=None, embedding_model_name=None, k=5):\n",
    "    # Initialize the evaluator with a pre-trained Sentence-BERT model\n",
    "    evaluator = EmbeddingEvaluator(model_type, model_path=model_path, embedding_model_name=embedding_model_name, k=k)\n",
    "\n",
    "    # Evaluate the model on all datasets\n",
    "    evaluator.evaluate(datasets)\n",
    "\n",
    "    # Retrieve stored metrics\n",
    "    return(evaluator.get_metrics_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6982ac7-2a0f-4d88-b83a-a00ed27b8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['/home/mcorradi/researchdrive/BetterEmbedData/bioasq_test_set.json', '/home/mcorradi/researchdrive/BetterEmbedData/pubmedqa_test_set.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df37d75-d848-47a2-b9c5-139c281680fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EmbeddingEvaluator' object has no attribute 'generate_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metrics_general \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpretrained\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43membedding_model_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBAAI/bge-base-en-v1.5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(datasets, model_type, model_path, embedding_model_name, k)\u001b[0m\n\u001b[1;32m      4\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m EmbeddingEvaluator(model_type, model_path\u001b[38;5;241m=\u001b[39mmodel_path, embedding_model_name\u001b[38;5;241m=\u001b[39membedding_model_name, k\u001b[38;5;241m=\u001b[39mk)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Evaluate the model on all datasets\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Retrieve stored metrics\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(evaluator\u001b[38;5;241m.\u001b[39mget_metrics_summary())\n",
      "Cell \u001b[0;32mIn[1], line 99\u001b[0m, in \u001b[0;36mEmbeddingEvaluator.evaluate\u001b[0;34m(self, dataset_file_paths)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Generate question embeddings or use questions as is for ColBERT\u001b[39;00m\n\u001b[1;32m     98\u001b[0m question_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_question_embeddings(questions)\n\u001b[0;32m---> 99\u001b[0m answer_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_embeddings\u001b[49m(answers) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolbert\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m answers\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Retrieve the top-k results for each question\u001b[39;00m\n\u001b[1;32m    102\u001b[0m top_k_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve_top_k(question_embeddings, answer_embeddings)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EmbeddingEvaluator' object has no attribute 'generate_embeddings'"
     ]
    }
   ],
   "source": [
    "metrics_general = evaluate_model(datasets,model_type='pretrained',embedding_model_name = 'BAAI/bge-base-en-v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92d1792f-b793-4027-bbad-cf304d64c355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: /home/mcorradi/researchdrive/BetterEmbedData/bioasq_test_set.json, MRR: 0.809, Precision@5: 0.590, Recall@5: 0.421\n",
      "Dataset: /home/mcorradi/researchdrive/BetterEmbedData/pubmedqa_test_set.json, MRR: 0.936, Precision@5: 0.195, Recall@5: 0.973\n"
     ]
    }
   ],
   "source": [
    "metrics_pubmed = evaluate_model(datasets,model_type='pretrained',embedding_model_name = 'neuml/pubmedbert-base-embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba031f1-5994-4786-b749-1e9ce872f06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Oct 30, 15:17:14] #> Note: Output directory .ragatouille/colbert/indexes/default already exists\n",
      "\n",
      "\n",
      "#> Starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcorradi/.conda/envs/embeddings/lib/python3.10/site-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "Process Process-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mcorradi/.conda/envs/embeddings/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mcorradi/.conda/envs/embeddings/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mcorradi/.conda/envs/embeddings/lib/python3.10/site-packages/colbert/infra/launcher.py\", line 134, in setup_new_process\n",
      "    return_val = callee(config, *args)\n",
      "  File \"/home/mcorradi/.conda/envs/embeddings/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py\", line 33, in encode\n",
      "    encoder.run(shared_lists)\n",
      "  File \"/home/mcorradi/.conda/envs/embeddings/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py\", line 63, in run\n",
      "    self.setup() # Computes and saves plan for whole collection\n",
      "  File \"/home/mcorradi/.conda/envs/embeddings/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py\", line 101, in setup\n",
      "    avg_doclen_est = self._sample_embeddings(sampled_pids)\n",
      "  File \"/home/mcorradi/.conda/envs/embeddings/lib/python3.10/site-packages/colbert/indexing/collection_indexer.py\", line 137, in _sample_embeddings\n",
      "    local_sample_embs, doclens = self.encoder.encode_passages(local_sample)\n",
      "  File \"/home/mcorradi/.conda/envs/embeddings/lib/python3.10/site-packages/colbert/indexing/collection_encoder.py\", line 26, in encode_passages\n",
      "    embs_, doclens_ = self.checkpoint.docFromText(\n",
      "  File \"/home/mcorradi/.conda/envs/embeddings/lib/python3.10/site-packages/colbert/modeling/checkpoint.py\", line 133, in docFromText\n",
      "    assert clustering_mode in [\"hierarchical\"]\n",
      "  File \"/home/mcorradi/.conda/envs/embeddings/lib/python3.10/site-packages/colbert/infra/config/core_config.py\", line 22, in __eq__\n",
      "    self.val == other.val\n",
      "AttributeError: 'str' object has no attribute 'val'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nranks = 1 \t num_gpus = 1 \t device=0\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"index_bsize\": 64,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"resume\": false,\n",
      "    \"pool_factor\": {\n",
      "        \"val\": 1\n",
      "    },\n",
      "    \"clustering_mode\": {\n",
      "        \"val\": \"hierarchical\"\n",
      "    },\n",
      "    \"protected_tokens\": {\n",
      "        \"val\": 0\n",
      "    },\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 1e-5,\n",
      "    \"maxsteps\": 15626,\n",
      "    \"save_every\": null,\n",
      "    \"warmup\": 781,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 32,\n",
      "    \"use_ib_negatives\": false,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": \"answerdotai\\/AnswerAI-ColBERTv2.5-small\",\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 96,\n",
      "    \"doc_maxlen\": 512,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"answerdotai\\/answerai-colbert-small-v1\",\n",
      "    \"triples\": \"\\/home\\/bclavie\\/colbertv2.5_en\\/data\\/msmarco\\/triplets.jsonl\",\n",
      "    \"collection\": [\n",
      "        \"list with 11986 elements starting with...\",\n",
      "        [\n",
      "            \"They combine two different monospecific antigen-binding regions that target CD20 on B cells and engage T cells via CD3 in a 1:1 or 2:1 CD20:CD3 antigen binding fragment (Fab) format. The results of different phase 1 trials with BsAbs, including mosunetuzumab, glofitamab, epcoritamab and odeonextamab, have been recently published.\",\n",
      "            \"Mosunetuzumab (Lunsumio\\u00ae), an anti-CD20\\/CD3 T-cell engaging bispecific antibody, is being developed by Roche for the treatment of relapsed or refractory follicular lymphoma. \",\n",
      "            \"BACKGROUND: Mosunetuzumab is a CD20\\u2009\\u00d7\\u2009CD3 T-cell-engaging bispecific monoclonal antibody that redirects T cells to eliminate malignant B cells. I\"\n",
      "        ]\n",
      "    ],\n",
      "    \"queries\": \"\\/home\\/bclavie\\/colbertv2.5_en\\/data\\/msmarco\\/queries.tsv\",\n",
      "    \"index_name\": \"default\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \".ragatouille\\/\",\n",
      "    \"experiment\": \"colbert\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-10\\/30\\/14.20.41\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 1,\n",
      "    \"avoid_fork_if_possible\": false\n",
      "}\n",
      "[Oct 30, 15:17:17] [0] \t\t # of sampled PIDs = 11986 \t sampled_pids[:3] = [6825, 166, 4892]\n",
      "[Oct 30, 15:17:17] [0] \t\t #> Encoding 11986 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1030 15:17:17.376733674 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "metrics_colbert = evaluate_model(datasets,model_type='colbert',embedding_model_name = 'answerdotai/answerai-colbert-small-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f43c63d-e5ec-41ff-9139-90209ccc3256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embeddings",
   "language": "python",
   "name": "embeddings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
